{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minor_project_ML_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "NUfkrYC72Yqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/aashishksahu/Minor_Project/master/data%20-%20data.csv?token=AZjmbbgwmS_1ps7EyeCXED29LQG_ufegks5cUVCGwA%3D%3D\"\n",
        "file = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgpw9Um83rmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-yazWPCeSCN",
        "colab_type": "code",
        "outputId": "88c7bb34-484e-4915-f7af-2ca641cea761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "cell_type": "code",
      "source": [
        "# visualising the data\n",
        "data = pd.DataFrame(file)\n",
        "# shuffling\n",
        "data = data.sample(frac=1)\n",
        "# shows the top 5 rows\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>8860702</td>\n",
              "      <td>M</td>\n",
              "      <td>17.30</td>\n",
              "      <td>17.08</td>\n",
              "      <td>113.00</td>\n",
              "      <td>928.2</td>\n",
              "      <td>0.10080</td>\n",
              "      <td>0.10410</td>\n",
              "      <td>0.12660</td>\n",
              "      <td>0.08353</td>\n",
              "      <td>...</td>\n",
              "      <td>19.85</td>\n",
              "      <td>25.09</td>\n",
              "      <td>130.90</td>\n",
              "      <td>1222.0</td>\n",
              "      <td>0.1416</td>\n",
              "      <td>0.2405</td>\n",
              "      <td>0.3378</td>\n",
              "      <td>0.18570</td>\n",
              "      <td>0.3138</td>\n",
              "      <td>0.08113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>873593</td>\n",
              "      <td>M</td>\n",
              "      <td>21.09</td>\n",
              "      <td>26.57</td>\n",
              "      <td>142.70</td>\n",
              "      <td>1311.0</td>\n",
              "      <td>0.11410</td>\n",
              "      <td>0.28320</td>\n",
              "      <td>0.24870</td>\n",
              "      <td>0.14960</td>\n",
              "      <td>...</td>\n",
              "      <td>26.68</td>\n",
              "      <td>33.48</td>\n",
              "      <td>176.50</td>\n",
              "      <td>2089.0</td>\n",
              "      <td>0.1491</td>\n",
              "      <td>0.7584</td>\n",
              "      <td>0.6780</td>\n",
              "      <td>0.29030</td>\n",
              "      <td>0.4098</td>\n",
              "      <td>0.12840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>901836</td>\n",
              "      <td>B</td>\n",
              "      <td>11.04</td>\n",
              "      <td>14.93</td>\n",
              "      <td>70.67</td>\n",
              "      <td>372.7</td>\n",
              "      <td>0.07987</td>\n",
              "      <td>0.07079</td>\n",
              "      <td>0.03546</td>\n",
              "      <td>0.02074</td>\n",
              "      <td>...</td>\n",
              "      <td>12.09</td>\n",
              "      <td>20.83</td>\n",
              "      <td>79.73</td>\n",
              "      <td>447.1</td>\n",
              "      <td>0.1095</td>\n",
              "      <td>0.1982</td>\n",
              "      <td>0.1553</td>\n",
              "      <td>0.06754</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.07287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>855167</td>\n",
              "      <td>M</td>\n",
              "      <td>13.44</td>\n",
              "      <td>21.58</td>\n",
              "      <td>86.18</td>\n",
              "      <td>563.0</td>\n",
              "      <td>0.08162</td>\n",
              "      <td>0.06031</td>\n",
              "      <td>0.03110</td>\n",
              "      <td>0.02031</td>\n",
              "      <td>...</td>\n",
              "      <td>15.93</td>\n",
              "      <td>30.25</td>\n",
              "      <td>102.50</td>\n",
              "      <td>787.9</td>\n",
              "      <td>0.1094</td>\n",
              "      <td>0.2043</td>\n",
              "      <td>0.2085</td>\n",
              "      <td>0.11120</td>\n",
              "      <td>0.2994</td>\n",
              "      <td>0.07146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>894326</td>\n",
              "      <td>M</td>\n",
              "      <td>18.22</td>\n",
              "      <td>18.87</td>\n",
              "      <td>118.70</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>0.09746</td>\n",
              "      <td>0.11170</td>\n",
              "      <td>0.11300</td>\n",
              "      <td>0.07950</td>\n",
              "      <td>...</td>\n",
              "      <td>21.84</td>\n",
              "      <td>25.00</td>\n",
              "      <td>140.90</td>\n",
              "      <td>1485.0</td>\n",
              "      <td>0.1434</td>\n",
              "      <td>0.2763</td>\n",
              "      <td>0.3853</td>\n",
              "      <td>0.17760</td>\n",
              "      <td>0.2812</td>\n",
              "      <td>0.08198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "253  8860702         M        17.30         17.08          113.00      928.2   \n",
              "181   873593         M        21.09         26.57          142.70     1311.0   \n",
              "381   901836         B        11.04         14.93           70.67      372.7   \n",
              "40    855167         M        13.44         21.58           86.18      563.0   \n",
              "317   894326         M        18.22         18.87          118.70     1027.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "253          0.10080           0.10410         0.12660              0.08353   \n",
              "181          0.11410           0.28320         0.24870              0.14960   \n",
              "381          0.07987           0.07079         0.03546              0.02074   \n",
              "40           0.08162           0.06031         0.03110              0.02031   \n",
              "317          0.09746           0.11170         0.11300              0.07950   \n",
              "\n",
              "              ...             radius_worst  texture_worst  perimeter_worst  \\\n",
              "253           ...                    19.85          25.09           130.90   \n",
              "181           ...                    26.68          33.48           176.50   \n",
              "381           ...                    12.09          20.83            79.73   \n",
              "40            ...                    15.93          30.25           102.50   \n",
              "317           ...                    21.84          25.00           140.90   \n",
              "\n",
              "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "253      1222.0            0.1416             0.2405           0.3378   \n",
              "181      2089.0            0.1491             0.7584           0.6780   \n",
              "381       447.1            0.1095             0.1982           0.1553   \n",
              "40        787.9            0.1094             0.2043           0.2085   \n",
              "317      1485.0            0.1434             0.2763           0.3853   \n",
              "\n",
              "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "253               0.18570          0.3138                  0.08113  \n",
              "181               0.29030          0.4098                  0.12840  \n",
              "381               0.06754          0.3202                  0.07287  \n",
              "40                0.11120          0.2994                  0.07146  \n",
              "317               0.17760          0.2812                  0.08198  \n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "1EgbLz7nehOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class util:\n",
        "    def min_max_scaler(self,arr):\n",
        "        return ( (arr - np.min(arr))/(np.max(arr) - np.min(arr)) )\n",
        "\n",
        "    # one_hot maker\n",
        "    def one_hot(self, vector):\n",
        "        one_hot_vec = np.zeros([len(vector), 2])\n",
        "    \n",
        "        for i in range(len(vector)):\n",
        "            one_hot_vec[i, vector[i][0]] = 1\n",
        "       \n",
        "        return one_hot_vec\n",
        "    \n",
        "    # sigmoid function\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "            z: input vector\n",
        "            \n",
        "            returns vector with sigmoid non-linearity function applied to it        \n",
        "        \"\"\"\n",
        "        e = 0.000001\n",
        "        z = np.float32(z)\n",
        "        return 1/(1+np.exp(-z)+e)\n",
        "    \n",
        "    # derivative of sigmoid\n",
        "    def sigmoid_grad(self, z):\n",
        "        \"\"\"\n",
        "            z: input vector\n",
        "            \n",
        "            returns vector with sigmoid non-linearity function applied to it        \n",
        "        \"\"\"\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jh_vFuthflhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NN_model:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def forward_pass(self, Input, weights):\n",
        "        \"\"\"\n",
        "            Input: input sample(s)\n",
        "            weights: dictionary holding the weights\n",
        "            \n",
        "            returns a dictionary of values calculated at each layer\n",
        "        \"\"\"\n",
        "        \n",
        "        #assigning weights\n",
        "        w1 = weights[\"w1\"]\n",
        "        w2 = weights[\"w2\"]\n",
        "        b1 = weights[\"b1\"]\n",
        "        b2 = weights[\"b2\"]\n",
        "        \n",
        "        \n",
        "        # network layers\n",
        "        dense_layer_1 = np.dot(Input, w1) + b1\n",
        "        dense_layer_2 = np.dot(dense_layer_1, w2) + b2\n",
        "        activation_layer = util().sigmoid(dense_layer_2)\n",
        "\n",
        "               \n",
        "        forward_pass_params = {\"Input\": Input, \"dense_layer_1\": dense_layer_1, \n",
        "                               \"dense_layer_2\": dense_layer_2, \"logits\": activation_layer}\n",
        "        \n",
        "        return forward_pass_params\n",
        "    \n",
        "    # function for backpropagation\n",
        "    def backward_pass(self, labels, params, weights):\n",
        "        \"\"\"\n",
        "            labels: label for each training set (1 or 0)\n",
        "            params: values calculated at each layer\n",
        "            weights: dictionary holding the weights\n",
        "            \n",
        "            returns updated weights \n",
        "        \"\"\"\n",
        "        \n",
        "        #assigning weights\n",
        "        w1 = weights[\"w1\"]\n",
        "        w2 = weights[\"w2\"]\n",
        "        b1 = weights[\"b1\"]\n",
        "        b2 = weights[\"b2\"]\n",
        "        \n",
        "        # assigning forward pass parameters\n",
        "        Input = params[\"Input\"]\n",
        "        l1 = params[\"dense_layer_1\"]\n",
        "        l2 = params[\"dense_layer_2\"]\n",
        "        a  = params[\"logits\"]\n",
        "        \n",
        "        # no. of training samples\n",
        "        n = l1.shape[0]\n",
        "        \n",
        "        \n",
        "        # backprop (basically chain rule)\n",
        "        d_loss_da  = -(1/n) * (a - labels)\n",
        "        \n",
        "        # dL/dw2 = dL/da -> da/dl2 -> dl2/dw2\n",
        "        d_loss_dw2 = np.dot( np.multiply(d_loss_da, util().sigmoid_grad(l2)).T, l1 ).T\n",
        "        # dL/dw2 = dL/da -> da/dl2 -> dl2/db2\n",
        "        d_loss_db2 = np.dot(d_loss_da.T, util().sigmoid_grad(l2))\n",
        "        \n",
        "        # dL/dw1 = dL/da -> da/dl2 -> dl2/dl1 -> dl1/dw1\n",
        "        d_loss_dl2 = np.multiply(d_loss_da, util().sigmoid_grad(l2))\n",
        "        d_loss_dl1 = np.dot(d_loss_dl2, w2.T)\n",
        "        d_loss_dw1 = np.dot(Input.T, d_loss_dl1) \n",
        "        \n",
        "        # dL/db1 = dL/da -> da/dl2 -> dl2/dl1 -> dl1/db1\n",
        "        d_loss_db1 = np.dot(np.ones([1,n]), d_loss_dl1)\n",
        "                \n",
        "        \n",
        "        # updating weights\n",
        "        w1 = w1 + self.learning_rate*d_loss_dw1\n",
        "        w2 = w2 + self.learning_rate*d_loss_dw2\n",
        "        b1 = b1 + self.learning_rate*d_loss_db1\n",
        "        b2 = b2 + self.learning_rate*d_loss_db2\n",
        "        \n",
        "   \n",
        "        \n",
        "        # storing updated weights in dictionary\n",
        "        weights[\"w1\"] = w1\n",
        "        weights[\"w2\"] = w2\n",
        "        weights[\"b1\"] = b1\n",
        "        weights[\"b2\"] = b2\n",
        "        \n",
        "        return weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LvZSHnap4tsa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_Loss(yhat, y):\n",
        "    \"\"\"\n",
        "        yhat: predctions \n",
        "        y   : actual labels\n",
        "        \n",
        "    \"\"\"\n",
        "    # calculate loss\n",
        "    Loss = -np.mean(  np.multiply(y,np.log(yhat)) + np.multiply((1-y), np.log(1-yhat)) )\n",
        "    \n",
        "    return Loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eOppjJ1BezMB",
        "colab_type": "code",
        "outputId": "6801468c-5a04-4e76-f5c7-27b228422a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# preparing data for input\n",
        "\n",
        "# fetching all the column names\n",
        "cl_names = list(data.keys())\n",
        "\n",
        "# replacing Malignant(M) and Benign(B) with 1 and 0 resp.\n",
        "diagnosis_flag = np.array(data[cl_names[1]])\n",
        "\n",
        "for i in range(len(diagnosis_flag)):\n",
        "    if diagnosis_flag[i] == \"M\":\n",
        "        diagnosis_flag[i] = 1\n",
        "    else:\n",
        "        diagnosis_flag[i] = 0\n",
        "\n",
        "# splitting ratio\n",
        "split = int(len(data)*0.8)\n",
        "\n",
        "# Training set\n",
        "in_train = np.array(data[cl_names[2:32]][0: split])\n",
        "target_train = diagnosis_flag[0:split]\n",
        "target_train = target_train.reshape(len(target_train), 1)\n",
        "in_train = util().min_max_scaler(in_train)\n",
        "\n",
        "\n",
        "# test set\n",
        "in_test = np.array(data[cl_names[2:32]][split:len(data)])\n",
        "target_test = diagnosis_flag[split: len(diagnosis_flag)]\n",
        "target_test = target_test.reshape(len(target_test), 1)\n",
        "in_test = util().min_max_scaler(in_test)\n",
        "\n",
        "\n",
        "print(\"input (train): \", in_train.shape)\n",
        "print(\"labels (train): \", target_train.shape)\n",
        "print(\"input (test): \", in_test.shape)\n",
        "print(\"labels (test): \", target_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input (train):  (455, 30)\n",
            "labels (train):  (455, 1)\n",
            "input (test):  (114, 30)\n",
            "labels (test):  (114, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4X7MhaIJe75Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialising weights from normal distribution\n",
        "weight_dict = {\"w1\": np.random.normal(0,1, [30,5]),\n",
        "               \"w2\": np.random.normal(0,1, [5,1]),\n",
        "               \"b1\": np.random.normal(0,1, [1, 5]),\n",
        "               \"b2\": np.random.normal(0,1, [1, 1])}\n",
        "\n",
        "# initialising training model\n",
        "nn = NN_model(0.45)\n",
        "\n",
        "# list to save loss \n",
        "loss_list = []\n",
        "\n",
        "# no. of iterations\n",
        "epochs = 2000\n",
        "\n",
        "# initialising training loop\n",
        "for i in range(epochs):\n",
        "    \n",
        "    # forward pass\n",
        "    forward_out = nn.forward_pass(in_train, weight_dict)\n",
        "    \n",
        "    # backward pass\n",
        "    weight_dict = nn.backward_pass(target_train, forward_out, weight_dict)\n",
        "    \n",
        "    loss_list.append(get_Loss(forward_out[\"logits\"], target_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efPoNUwsGKds",
        "colab_type": "code",
        "outputId": "e348bf85-3a85-4708-c5e2-f8a9234548be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(range(0, epochs), loss_list)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nifx/W5RghACR5tIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "duKB9md3AIK4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function checks for true positives and true negatives\n",
        "def true_checker(pred, org):\n",
        "    \"\"\"\n",
        "        pred: prediction vector\n",
        "        \n",
        "        returns 1 for true positives and true negatives, 0 otherwise\n",
        "    \"\"\"\n",
        "    # convert probabilities into discrete signals (0 and 1)\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] > 0.5:\n",
        "            pred[i] = 1\n",
        "        else:\n",
        "            pred[i] = 0\n",
        "    \n",
        "    # checks for eqality\n",
        "    true_checker_list = [int(pred[i] == org[i]) for i in range(len(org))]\n",
        "    return true_checker_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbKSXonTqcnX",
        "colab_type": "code",
        "outputId": "7befbdff-4ef8-4bf1-c262-54e8c6d4a58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# training accuracy\n",
        "train_pred = nn.forward_pass(in_train, weight_dict)\n",
        "train_pred = train_pred[\"logits\"]\n",
        "true_checks_train = true_checker(train_pred, target_train)\n",
        "train_accuracy = np.sum(true_checks_train)/len(train_pred) * 100\n",
        "print(\"training set accuracy: \", train_accuracy, \"%\")\n",
        "\n",
        "# testing accuracy\n",
        "test_pred = nn.forward_pass(in_test, weight_dict)\n",
        "test_pred = test_pred[\"logits\"]\n",
        "true_checks_test = true_checker(test_pred, target_test)\n",
        "test_accuracy = np.sum(true_checks_test)/len(test_pred) * 100\n",
        "print(\"testing set accuracy: \", test_accuracy, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set accuracy:  90.76923076923077 %\n",
            "testing set accuracy:  91.22807017543859 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
